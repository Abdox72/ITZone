from fastapi import APIRouter, File, UploadFile, Depends
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from faster_whisper import WhisperModel
import torch
import tempfile
import os
from typing import Dict, Any
from sqlalchemy.orm import Session

from database import get_db
from security import get_current_active_user
import models

router = APIRouter(
    prefix="/audio",
    tags=["audio"],
    responses={404: {"description": "Not found"}},
)

# ğŸ” ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…ØªØ§Ø­ (GPU Ø£Ùˆ CPU)
use_cuda = torch.cuda.is_available()
device = "cuda" if use_cuda else "cpu"
torch_dtype = torch.float16 if use_cuda else torch.float32

print(f"âœ… Using device: {device}")

# ğŸ§ ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Whisper (ØµÙˆØª â†’ Ù†Øµ)
whisper_model = WhisperModel("base", device=device, compute_type="float16" if use_cuda else "int8")

# ğŸ¤– ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Qwen Chat
model_name = "Qwen/Qwen1.5-1.8B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto" if use_cuda else None,
    torch_dtype=torch_dtype,
    trust_remote_code=True
)

# âœ… Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0 if use_cuda else -1)

# âœ¨ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù€ prompt
def generate_prompt(transcript: str) -> str:
    return f"""
Ù„Ø¯ÙŠÙƒ Ù†Øµ Ù…ÙƒØªÙˆØ¨ Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ ÙØ±ÙŠÙ‚ Ø¹Ù…Ù„ (Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©).

Ù…Ø·Ù„ÙˆØ¨:
1. ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©.
2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù‡Ø§Ù… ÙˆØ§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠÙ† Ø¹Ù†Ù‡Ø§.
3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (deadlines).
4. ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù….

Ù†Øµ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹:
\"\"\"{transcript}\"\"\"
"""

@router.post("/analyze-meeting/", response_model=Dict[str, Any])
async def analyze_meeting_audio(
    file: UploadFile = File(...),
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_active_user)
):
    # ğŸ“¥ Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¤Ù‚ØªÙ‹Ø§
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
        tmp.write(await file.read())
        audio_path = tmp.name

    # ğŸ“ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
    segments, _ = whisper_model.transcribe(audio_path)
    transcript = " ".join([segment.text for segment in segments])
    os.remove(audio_path)

    # ğŸ§  ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Qwen
    prompt = generate_prompt(transcript)
    result = pipe(prompt, max_new_tokens=512, do_sample=False)[0]["generated_text"]

    return {
        "transcript": transcript,
        "analysis": result
    }